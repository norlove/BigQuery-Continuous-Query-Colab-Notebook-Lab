{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMQCs0oQf5Jo"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR0jdheRGG89"
      },
      "source": [
        "# BigQuery Streaming Reverse ETL into Pub/Sub\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiKsD6FM5i0m"
      },
      "source": [
        "| Author |\n",
        "| --- |\n",
        "| [Nick Orlove](https://github.com/norlove) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro_md"
      },
      "source": [
        "## Overview\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook provides a hands-on example of BigQuery's capability to perform **Streaming Reverse ETL into Pub/Sub**. You'll learn how BigQuery can continuously query data from a source table and stream the results to a Pub/Sub topic in near real-time using a [BigQuery continuous query](https://cloud.google.com/bigquery/docs/continuous-queries-introduction).\n",
        "\n",
        "Traditionally, ETL (Extract, Transform, Load) moves data into a data warehouse. Reverse ETL, on the other hand, moves data *out* of the data warehouse to operational systems or applications.\n",
        "\n",
        "This notebook demonstrates how BigQuery's continuous query feature, combined with Pub/Sub, enables a powerful Streaming Reverse ETL pattern for real-time event-based handling of data. This allows for immediate consumption of transformed or filtered data by downstream services, enabling real-time analytics, personalized experiences, or operational alerts based on the freshest data in BigQuery."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwQLISjqGTHd"
      },
      "source": [
        "### Objectives\n",
        "\n",
        "You will learn to:\n",
        "\n",
        "*   Set up a BigQuery continuous query and CONTINUOUS slot reservation.\n",
        "*   Stream data from BigQuery to a Pub/Sub topic in near real-time.\n",
        "*   Understand the concept of Streaming Reverse ETL and its applications.\n",
        "*   Verify the streamed data is successfully arriving in Pub/Sub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "costs_md"
      },
      "source": [
        "### Services and Costs\n",
        "\n",
        "This tutorial uses the following billable components of Google Cloud:\n",
        "\n",
        "* **BigQuery**: [Pricing](https://cloud.google.com/bigquery/pricing)\n",
        "\n",
        "* **Pub/Sub**: [Pricing](https://cloud.google.com/pubsub/pricing)\n",
        "\n",
        "You can use the [Pricing Calculator](https://cloud.google.com/products/calculator) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkoCFoFVSPii"
      },
      "source": [
        "---\n",
        "\n",
        "## Before you begin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_md_1"
      },
      "source": [
        "### Set up your Google Cloud project\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "3. [Enable the BigQuery and Pub/Sub APIs](https://console.cloud.google.com/flows/enableapi?apiid=bigquery.googleapis.com,pubsub.googleapis.com).\n",
        "\n",
        "4. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install and upgrade the `bigquery-magics` and `bigframes` libraries. `bigquery-magics` provides convenient IPython magic commands for BigQuery, while `bigframes` enables scalable data analysis with a DataFrame API directly on BigQuery data.\n"
      ],
      "metadata": {
        "id": "qi_4egwFSkwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade bigquery-magics\n",
        "%pip install --upgrade bigframes\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_MKAfST5SnyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ3g-h7uTaSf"
      },
      "source": [
        "### Set your project ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_project_id"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"nickorlove-demos\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auth_md"
      },
      "source": [
        "### Authenticate to your Google Cloud account\n",
        "\n",
        "Depending on your Jupyter environment, you may have to manually authenticate. Follow the relevant instructions below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6NjZRCXU5Ro"
      },
      "source": [
        "**1. Colab Enterprise in BigQuery Studio or Vertex AI**\n",
        "* Do nothing as you are already authenticated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0dV1hvAU1ed"
      },
      "source": [
        "**2. Colab Consumer - uncomment and run the following:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auth_code"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Pp4LAJ3UyRP"
      },
      "source": [
        "**3. Local JupyterLab instance, uncomment and run the following:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzU7S3fMVDkW"
      },
      "outputs": [],
      "source": [
        "# ! gcloud auth login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "perms_md"
      },
      "source": [
        "### Create a Service Account and set permissions for it\n",
        "\n",
        "In order to run a continuous query with exports to Pub/Sub, you must use a service account. For more information, see [continuous queries documentation](https://cloud.google.com/bigquery/docs/continuous-queries#choose_an_account_type).\n",
        "\n",
        "Keep in mind, to submit a job that runs using a service account, the user account must have the [Service Account User (roles/iam.serviceAccountUser)](https://cloud.google.com/iam/docs/service-account-permissions#user-role) role."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "perms_code"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "\n",
        "# Define a unique name for the service account\n",
        "SERVICE_ACCOUNT_ID = \"bq-continuous-query-demo-sa\"\n",
        "\n",
        "# Construct the full email address for the service account\n",
        "SERVICE_ACCOUNT_EMAIL = f\"{SERVICE_ACCOUNT_ID}@{PROJECT_ID}.iam.gserviceaccount.com\"\n",
        "\n",
        "print(f\"Creating service account: {SERVICE_ACCOUNT_ID}...\")\n",
        "\n",
        "# Create the service account using gcloud\n",
        "! gcloud iam service-accounts create {SERVICE_ACCOUNT_ID} \\\n",
        "    --display-name=\"BigQuery continuous query to Pub/Sub Export Demo\" \\\n",
        "    --description=\"Service account for the BigQuery continuous query to Pub/Sub tutorial\" > /dev/null 2>&1\n",
        "\n",
        "print(f\"\\nAssigning necessary IAM roles to {SERVICE_ACCOUNT_EMAIL}...\")\n",
        "\n",
        "# Assign the BigQuery User role\n",
        "! gcloud projects add-iam-policy-binding {PROJECT_ID} \\\n",
        "    --member=\"serviceAccount:{SERVICE_ACCOUNT_EMAIL}\" \\\n",
        "    --role=\"roles/bigquery.user\" \\\n",
        "    --condition=None \\\n",
        "    --quiet > /dev/null 2>&1\n",
        "\n",
        "# Assign the BigQuery Data Editor role\n",
        "! gcloud projects add-iam-policy-binding {PROJECT_ID} \\\n",
        "    --member=\"serviceAccount:{SERVICE_ACCOUNT_EMAIL}\" \\\n",
        "    --role=\"roles/bigquery.dataEditor\" \\\n",
        "    --condition=None \\\n",
        "    --quiet > /dev/null 2>&1\n",
        "\n",
        "# Assign the Pub/Sub Publisher role\n",
        "! gcloud projects add-iam-policy-binding {PROJECT_ID} \\\n",
        "    --member=\"serviceAccount:{SERVICE_ACCOUNT_EMAIL}\" \\\n",
        "    --role=\"roles/pubsub.publisher\" \\\n",
        "    --condition=None \\\n",
        "    --quiet > /dev/null 2>&1\n",
        "\n",
        "# Assign the Pub/Sub Viewer role\n",
        "! gcloud projects add-iam-policy-binding {PROJECT_ID} \\\n",
        "    --member=\"serviceAccount:{SERVICE_ACCOUNT_EMAIL}\" \\\n",
        "    --role=\"roles/pubsub.viewer\" \\\n",
        "    --condition=None \\\n",
        "    --quiet > /dev/null 2>&1\n",
        "\n",
        "print(f\"\\n✅ Successfully created service account and assigned permissions.\")\n",
        "\n",
        "# Wait ~60 seconds, to give IAM updates time to propagate. Otherwise, subsequent cells may fail.\n",
        "time.sleep(60)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Pub/Sub topic and subscription for the continuous query to write to.\n",
        "\n",
        "Next, we need a destination for our continuous query's output. We'll create a Pub/Sub topic, which is where the query will stream data to.\n",
        "\n",
        "To verify that data is arriving correctly, we'll also create a Pub/Sub subscription. A subscription allows us to listen to the messages sent to the topic. Later, we'll use this subscription to pull messages and confirm our end-to-end pipeline is working."
      ],
      "metadata": {
        "id": "HEKljbZxMBT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define unique names for your Pub/Sub topic and subscription.\n",
        "TOPIC_ID = \"cq_results_topic\"\n",
        "SUBSCRIPTION_ID = \"cq_results_viewer_sub\"\n",
        "\n",
        "print(f\"Creating Pub/Sub topic: {TOPIC_ID}...\")\n",
        "\n",
        "# Create the Pub/Sub topic using the gcloud CLI\n",
        "! gcloud pubsub topics create {TOPIC_ID}\n",
        "\n",
        "print(f\"\\nCreating Pub/Sub subscription: {SUBSCRIPTION_ID}...\")\n",
        "\n",
        "# Create a subscription attached to the topic\n",
        "! gcloud pubsub subscriptions create {SUBSCRIPTION_ID} --topic={TOPIC_ID}\n",
        "\n",
        "print(f\"\\n✅ Successfully created topic '{TOPIC_ID}' and subscription '{SUBSCRIPTION_ID}'.\")"
      ],
      "metadata": {
        "id": "yWKrcj-fMAh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fhqCNYQOLCq"
      },
      "source": [
        "### Create a Source BigQuery Dataset and Table\n",
        "\n",
        "Now that we have a destination for our data (the Pub/Sub topic), we need a source. A continuous query reads data from a BigQuery table as new rows are added."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZR9S5lgpOGDr"
      },
      "outputs": [],
      "source": [
        "%%bigquery --project {PROJECT_ID}\n",
        "\n",
        "-- This statement creates a new dataset to hold our source table.\n",
        "CREATE SCHEMA IF NOT EXISTS `cq_source_dataset`\n",
        "OPTIONS (\n",
        "  location = 'US',\n",
        "  description = 'Dataset for the continuous query source table.'\n",
        ");\n",
        "\n",
        "-- This statement creates the source table with a defined schema.\n",
        "CREATE OR REPLACE TABLE `cq_source_dataset.user_clicks`\n",
        "(\n",
        "  event_timestamp TIMESTAMP NOT NULL OPTIONS(description=\"The exact time of the user event.\"),\n",
        "  user_id STRING NOT NULL OPTIONS(description=\"The unique identifier for the user.\"),\n",
        "  product_id STRING OPTIONS(description=\"The identifier for the product clicked.\"),\n",
        "  value FLOAT64 OPTIONS(description=\"The value of the product the user clicked on.\")\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2xG-u0qaLuG"
      },
      "source": [
        "### Create a BigQuery CONTINUOUS Slot Reservation\n",
        "\n",
        "Continuous queries must run in their own dedicated BigQuery reservation with a CONTINUOUS job type. You can't run them using the on-demand billing model.\n",
        "\n",
        "A reservation is a dedicated pool of BigQuery processing power (slots). For this demo, we'll create a reservation that uses autoscaling. It will have 0 baseline slots and will automatically scale up to a maximum of 50 slots as needed to run our query. An assignment links this reservation to our project, telling BigQuery to use this specific slot pool for any jobs of type CONTINUOUS.\n",
        "\n",
        "**Note**: After a continuous query starts running, it actively listens for incoming data, which consumes slot resources. While a reservation with a running continuous query does not scale down to zero slots, an idle continuous query that is primarily listening for incoming data is expected to consume a minimal amount of slots, typically around 1 slot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JY3WhVBKT8c"
      },
      "outputs": [],
      "source": [
        "%%bigquery --project {PROJECT_ID} --pyformat\n",
        "\n",
        "-- This statement creates a new reservation with 0 baseline slots\n",
        "-- and the ability to autoscale up to 50 slots.\n",
        "CREATE RESERVATION `region-US.cq-demo-reservation`\n",
        "OPTIONS(\n",
        "  edition = 'ENTERPRISE',\n",
        "  slot_capacity = 0, -- Baseline slots\n",
        "  autoscale_max_slots = 50\n",
        ");\n",
        "\n",
        "-- This statement assigns the reservation to the current project specifically for\n",
        "-- continuous query jobs. The job_type MUST be 'CONTINUOUS'.\n",
        "CREATE ASSIGNMENT `region-US.cq-demo-reservation.cq-assignment`\n",
        "OPTIONS(\n",
        "  assignee = 'projects/{PROJECT_ID}',\n",
        "  job_type = 'CONTINUOUS'\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wait ~180 seconds, to give the BigQuery reservation time to propagate. Otherwise, subsequent cells may fail.\n"
      ],
      "metadata": {
        "id": "8LzLF8W8-aFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "time.sleep(180)"
      ],
      "metadata": {
        "id": "3ji1DP-U-T3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_bbC8thoHGU"
      },
      "source": [
        "### Start inserting some random data into the source table\n",
        "\n",
        "The following cell streams some data into BigQuery. It:\n",
        "\n",
        "1. Generates a random user ID, product ID, and value.\n",
        "\n",
        "2. Captures the current timestamp.\n",
        "\n",
        "3. Inserts this data as a new row into the user_clicks table.\n",
        "\n",
        "This will insert 100 records into the BigQuery table.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLqRk757KT3O"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from datetime import datetime\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# --- Configuration ---\n",
        "# Ensure your PROJECT_ID variable is set correctly in your notebook environment\n",
        "# PROJECT_ID = \"your-gcp-project-id\"\n",
        "TABLE_ID = f\"{PROJECT_ID}.cq_source_dataset.user_clicks\"\n",
        "NUM_RECORDS = 100\n",
        "\n",
        "# --- Client and Data Setup ---\n",
        "client = bigquery.Client()\n",
        "sample_user_ids = [f\"user_{i}\" for i in range(100, 120)]\n",
        "sample_product_ids = [f\"product_{i}\" for i in range(2000, 2050)]\n",
        "rows_to_insert = []\n",
        "\n",
        "print(f\"Generating {NUM_RECORDS} synthetic records in memory...\")\n",
        "\n",
        "# 1. Create all 100 records and add them to a list\n",
        "for _ in range(NUM_RECORDS):\n",
        "    row = {\n",
        "        \"user_id\": random.choice(sample_user_ids),\n",
        "        \"product_id\": random.choice(sample_product_ids),\n",
        "        \"event_timestamp\": datetime.utcnow().isoformat() + \"Z\",  # 'Z' for UTC timezone\n",
        "        \"value\": round(random.uniform(0, 100), 2),\n",
        "    }\n",
        "    rows_to_insert.append(row)\n",
        "\n",
        "print(\"Generation complete. Inserting rows into BigQuery as a single batch...\")\n",
        "\n",
        "# 2. Insert all records in one API call\n",
        "errors = client.insert_rows_json(TABLE_ID, rows_to_insert)\n",
        "\n",
        "# 3. Report the final result\n",
        "if not errors:\n",
        "    print(f\"✅ Successfully inserted {len(rows_to_insert)} rows into the table.\")\n",
        "else:\n",
        "    print(f\"❌ Encountered errors while inserting rows: {errors}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run a basic query against our table to varify it is receiving data."
      ],
      "metadata": {
        "id": "yVZM4GqiT13G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery --project {PROJECT_ID}\n",
        "\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  `cq_source_dataset.user_clicks`\n",
        "ORDER BY event_timestamp DESC\n",
        "LIMIT 10;"
      ],
      "metadata": {
        "id": "zADTlC6uT8qA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create and Start the Continuous Query\n",
        "\n",
        "A continuous query is a special type of BigQuery job that you define once and it runs continuously in the background.\n",
        "\n",
        "This cell will start a continuous query job using the BigQuery DataFrames library. It takes the variables defined in previous cells—your PROJECT_ID, SERVICE_ACCOUNT_EMAIL, and TOPIC_ID—and uses them to create a live, streaming job."
      ],
      "metadata": {
        "id": "lqq4TuS8UKxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bigframes\n",
        "import bigframes.pandas as bpd\n",
        "import bigframes.streaming as bst\n",
        "\n",
        "bigframes.options._bigquery_options.project = PROJECT_ID\n",
        "job_id_prefix = \"bq_cq_notebook_\" #Set the job prefix for your Job ID so that it is easy to find\n",
        "\n",
        "#Create the StreamingDataFrame from a BigQuery table, select certain columns, filter rows and preview the output\n",
        "sdf = bst.read_gbq_table(\"cq_source_dataset.user_clicks\")\n",
        "\n",
        "sdf = sdf[[\"event_timestamp\",\"user_id\", \"product_id\", \"value\"]]\n",
        "sdf = sdf[sdf[\"value\"] > 50]\n",
        "\n",
        "iso_format_string = '%Y-%m-%dT%H:%M:%E6S%z'\n",
        "event_timestamp_str = sdf[\"event_timestamp\"].dt.strftime(iso_format_string)\n",
        "user_id_str = sdf[\"user_id\"].astype(str)\n",
        "product_id_str = sdf[\"product_id\"].astype(str)\n",
        "value_str = sdf[\"value\"].astype(str)\n",
        "\n",
        "sdf[\"data\"] = (\n",
        "    '{\"timestamp\": \"'\n",
        "    + event_timestamp_str\n",
        "    + '\", \"user_id\": \"'\n",
        "    + user_id_str\n",
        "    + '\", \"product_id\": \"'\n",
        "    + product_id_str\n",
        "    + '\", \"value\": \"'\n",
        "    + value_str\n",
        "    + '\"}'\n",
        ")\n",
        "\n",
        "# 3. Select only the new \"data\" column to send to Pub/Sub.\n",
        "sdf = sdf[[\"data\"]]\n",
        "\n",
        "job = sdf.to_pubsub(\n",
        "        topic=TOPIC_ID,\n",
        "        service_account_email=SERVICE_ACCOUNT_EMAIL,\n",
        "        job_id=None,\n",
        "        job_id_prefix=job_id_prefix,\n",
        "    )"
      ],
      "metadata": {
        "id": "-Gegbj07F6g-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirm the continuous query has successfully started."
      ],
      "metadata": {
        "id": "nT_042Ss5tPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(job.running())\n",
        "print(job.error_result)\n",
        "\n",
        "# Wait ~180 seconds, to give the BigQuery continuous query time to start up and begin to process incoming data.\n",
        "# Otherwise, subsequent cells may fail.\n",
        "time.sleep(180)"
      ],
      "metadata": {
        "id": "bdQemi6erhpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read from Pub/Sub\n",
        "\n",
        "Let's manually pull data from our Pub/Sub subscription to verify the continuous query is successfully writing data to it.\n",
        "\n",
        "**NOTE**: It may take a couple of minutes for the continuous query to fully start up and begin to process incoming data."
      ],
      "metadata": {
        "id": "FKFQHrs-F8LS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import pubsub_v1\n",
        "from concurrent.futures import TimeoutError\n",
        "import json\n",
        "\n",
        "# --- Configuration ---\n",
        "# How long to listen for messages\n",
        "timeout = 30.0\n",
        "# List to store received messages\n",
        "received_messages = []\n",
        "# Maximum number of messages to process before stopping\n",
        "max_messages_to_process = 10\n",
        "\n",
        "# Create a SubscriberClient\n",
        "subscriber = pubsub_v1.SubscriberClient()\n",
        "subscription_path = subscriber.subscription_path(PROJECT_ID, SUBSCRIPTION_ID)\n",
        "\n",
        "def callback(message):\n",
        "    \"\"\"A simple function called for each message received.\"\"\"\n",
        "    try:\n",
        "        # Decode the message data from bytes to a string\n",
        "        decoded_data = message.data.decode('utf-8')\n",
        "        print(f\"  -> Received raw message: {decoded_data}\")\n",
        "        # Append the raw string to our list\n",
        "        received_messages.append(decoded_data)\n",
        "        # Acknowledge the message so it's not sent again\n",
        "        message.ack()\n",
        "\n",
        "        # Stop listening if we've collected enough messages\n",
        "        if len(received_messages) >= max_messages_to_process:\n",
        "            streaming_pull_future.cancel()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing message: {e}\")\n",
        "        message.nack()\n",
        "\n",
        "print(f\"Listening for messages on {subscription_path}...\")\n",
        "print(f\"(Will stop after {max_messages_to_process} messages or {timeout} seconds)\")\n",
        "\n",
        "# The subscribe() method creates a background thread to pull messages.\n",
        "streaming_pull_future = subscriber.subscribe(subscription_path, callback=callback)\n",
        "\n",
        "# Wrap the call in a try/except block to handle the timeout.\n",
        "with subscriber:\n",
        "    try:\n",
        "        # This is a blocking call, waiting for the future to complete.\n",
        "        streaming_pull_future.result(timeout=timeout)\n",
        "    except TimeoutError:\n",
        "        streaming_pull_future.cancel()  # Trigger the shutdown.\n",
        "        streaming_pull_future.result()  # Block until the shutdown is complete.\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        streaming_pull_future.cancel()\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(f\"Finished listening. Collected {len(received_messages)} messages.\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "if received_messages:\n",
        "    # Simply print the raw messages as they came in\n",
        "    for i, msg_str in enumerate(received_messages):\n",
        "        print(f\"Message {i+1}: {msg_str}\")\n",
        "else:\n",
        "    print(\"No messages were collected.\")"
      ],
      "metadata": {
        "id": "YwrTAw3iF9jR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxM1hI7X5RXU"
      },
      "source": [
        "### Recap\n",
        "\n",
        "In this notebook, you have successfully implemented a **BigQuery Streaming Reverse ETL** pipeline to Pub/Sub. You learned how to:\n",
        "\n",
        "*   Set up necessary Google Cloud resources, including a service account, Pub/Sub topic and subscription, and a BigQuery source table.\n",
        "*   Create a dedicated BigQuery CONTINUOUS slot reservation to run the continuous query.\n",
        "*   Simulate a real-time data stream into the BigQuery source table.\n",
        "*   Define and start a BigQuery continuous query using the BigFrames library to filter and transform data.\n",
        "*   Stream the results of the continuous query to a Pub/Sub topic.\n",
        "*   Verify the data ingestion into Pub/Sub by pulling messages from the subscription.\n",
        "\n",
        "This demonstrates a powerful pattern for delivering fresh, processed data from BigQuery to downstream operational systems and applications in real-time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_A4zzWousGx"
      },
      "source": [
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cleanup_md"
      },
      "source": [
        "# Cleaning Up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRzpOQf_Wqqi"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import threading\n",
        "\n",
        "# Stop the continuous query if it's running\n",
        "# Check if 'job' in local scope and is a Job object from bigframes.streaming\n",
        "if 'job' in locals() and hasattr(job, 'running') and job.running():\n",
        "    print(\"Stopping continuous query...\")\n",
        "    job.cancel()\n",
        "    print(\"Continuous query stopped.\")\n",
        "\n",
        "# Delete the BigQuery dataset (which includes the table)\n",
        "# The dataset name is 'cq_source_dataset' from cell 'ZR9S5lgpOGDr'\n",
        "print(f\"Deleting BigQuery dataset {PROJECT_ID}.cq_source_dataset...\")\n",
        "! bq rm -r -f {PROJECT_ID}:cq_source_dataset\n",
        "\n",
        "# Delete Pub/Sub subscription\n",
        "# The subscription ID is 'cq_results_viewer_sub' from cell 'yWKrcj-fMAh7'\n",
        "print(f\"Deleting Pub/Sub subscription {SUBSCRIPTION_ID}...\")\n",
        "! gcloud pubsub subscriptions delete {SUBSCRIPTION_ID} --quiet\n",
        "\n",
        "# Delete Pub/Sub topic\n",
        "# The topic ID is 'cq_results_topic' from cell 'yWKrcj-fMAh7'\n",
        "print(f\"Deleting Pub/Sub topic {TOPIC_ID}...\")\n",
        "! gcloud pubsub topics delete {TOPIC_ID} --quiet\n",
        "\n",
        "# Delete the BigQuery reservation assignment\n",
        "# The assignment name is 'cq-assignment' from cell '1JY3WhVBKT8c'\n",
        "print(f\"Deleting BigQuery reservation assignment projects/{PROJECT_ID}/locations/US/reservations/cq-demo-reservation/assignments/cq-assignment...\")\n",
        "! bq rm --location=us --project_id={PROJECT_ID} --reservation_assignment cq-demo-reservation.cq-assignment\n",
        "\n",
        "# Delete the BigQuery reservation\n",
        "# The reservation name is 'cq-demo-reservation' from cell '1JY3WhVBKT8c'\n",
        "print(f\"Deleting BigQuery reservation projects/{PROJECT_ID}/locations/US/reservations/cq-demo-reservation...\")\n",
        "! bq rm --location=us --project_id={PROJECT_ID} --reservation cq-demo-reservation\n",
        "\n",
        "# Delete the service account\n",
        "# The service account email is 'SERVICE_ACCOUNT_EMAIL' from cell 'perms_code'\n",
        "print(f\"Deleting service account {SERVICE_ACCOUNT_EMAIL}...\")\n",
        "! gcloud iam service-accounts delete {SERVICE_ACCOUNT_EMAIL} --quiet\n",
        "\n",
        "print(\"✅ Cleanup complete.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "auth_md"
      ],
      "provenance": [],
      "toc_visible": true,
      "name": "BigQuery_Streaming_Reverse_ETL_to_PubSub"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}